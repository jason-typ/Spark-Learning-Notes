### 宽依赖与窄依赖

> The most interesting question in designing this interface is how to represent dependencies between RDDs. We found it both sufficient and useful to classify dependencies into two types: narrow dependencies, where each partition of the parent RDD is used by at most one partition of the child RDD, wide dependencies, where multiple child partitions may depend on it. For example, map leads to a narrow dependency, while join leads to to wide dependencies (unless the parents are hash-partitioned).

> This distinction is useful for two reasons. First, narrow dependencies allow for pipelined execution on one cluster node, which can compute all the parent partitions. For example, one can apply a map followed by a filter on an element-by-element basis. In contrast, wide dependencies require data from all parent partitions to be available and to be shuffled across the nodes using a MapReduce-like operation. Second, recovery after a node failure is more efficient with a narrow dependency, as only the lost parent partitions need to be recomputed, and they can be recomputed in parallel on different nodes. In contrast, in a lineage graph with wide dependencies, a single failed node might cause the loss of some partition from all the ancestors of an RDD, requiring a complete re-execution.

- 窄依赖：父RDD的一个分区至多被子RDD的一个分区所依赖
- 宽依赖：父RDD的一个分区被子RDD的多个分区所依赖

![宽依赖与窄依赖](https://huajianmao.github.io/assets/images/spark/Dependency.png)

这样划分的原因有两个：
1. 允许在单个节点上使用一个task流水化的执行。例如，在RDD上执行map操作后接着执行filter操作，RDD的每个Partition上都会启动一个Task，各个Task之间并行执行、互不影响，从而大大提高了执行效率。而对于宽依赖来说，它要求父RDD的所有Partition都已准备好，并且会在节点之间执行一个MapReduce类似的操作
2. 在一个节点失效时，如果节点上的RDD的分区是窄依赖的，那么只需要计算丢失的分区对应的父RDD的分区即可，并且可以在不同节点上并行执行。而对于宽依赖分区的失效，可能会造成全部数据的重新计算。

上面介绍的比较宽泛，具体理解起来不那么容易。特别是对于`join`操作，一会儿是窄依赖，一会儿是宽依赖的。总之，目前先了解宽依赖与窄依赖的区分标准：是否会发生Shuffle。在Spark中，Shuffle又可以理解为repartition，即数据的重新分区。简言之，**重新分区(即父RDD的某个分区的数据流入到了子RDD的多余一个的分区中)会发生Shuffle，就是宽依赖；不重新分区(父RDD一个分区中的数据只会流入到子RDD的一个分区中)不会发生Shuffle，就是窄依赖**。具体为什么会这样，需要了解Shuffle的执行过程。

另外，看图中，窄依赖还可以分为两种类型：
- **一对一**，父RDD的一个Partition对应子RDD的一个Partition
- **多对一**，父RDD的多个Partition对应子RDD的一个Partition

具体的类：
```
abstract class Dependency[T] extends Serializable
abstract class NarrowDependency[T](_rdd: RDD[T]) extends Dependency[T]
class ShuffleDependency[K: ClassTag, V: ClassTag, C: ClassTag](
    @transient private val _rdd: RDD[_ <: Product2[K, V]],
    val partitioner: Partitioner,
    val serializer: Serializer = SparkEnv.get.serializer,
    val keyOrdering: Option[Ordering[K]] = None,
    val aggregator: Option[Aggregator[K, V, C]] = None,
    val mapSideCombine: Boolean = false)
  extends Dependency[Product2[K, V]]
class OneToOneDependency[T](rdd: RDD[T]) extends NarrowDependency[T](rdd)
class RangeDependency[T](rdd: RDD[T], inStart: Int, outStart: Int, length: Int)
  extends NarrowDependency[T](rdd)
```

### Spark Shuffle

因此对于宽依赖来说，父RDD一个Partition上运行的task可能会更新子RDD的任意一个分区，所以需要父RDD的每个Partition上运行的task都结束，才可以继续下一个Stage。

#### 为什么co-partitioned join是窄依赖
co-partitioned RDD是指使用相同的partitioner(分区器)进行分区的两个或多个RDD。使用相同的分区器决定了RDD最终的分区数是相同的(一个没有数据的分区也算一个)，相同的key会被分到相同的(编号的)Partition中。
co-partitioned join指co-partitioned RDD之间的join操作。由于没有重新分区操作，即没有Shuffle，父RDD的任意一个分区里的数据都只流向了子RDD的一个分区中，因此，根据概念它确实是窄依赖。

首先需要明确Spark中Shuffle的概念：Shuffle意味着重新分区，Shuffle往往伴随着数据的网络传输，但有网络传输的操作不一定是Shuffle。根据Shuffle的介绍，我们知道Shuffle其实就是对数据的重新分区，并且过程分为两个步骤：
- Shuffle Write，单个task将父RDD的一个Partition的数据根据新的分区规则写到不同的文件中(即不同的分区)
- Shuffle Read，读取生成的文件(即分区)，作为下一个Stage的最开始的RDD

上面对于写文件-读文件的操作，与Hadoop中的Map-Reduce操作很类似。不过当内存足够大，数据足够小的时候，Shuffle的读写操作可以直接在内存中完成。

对于co-partitioned join操作，由于RDD的各个Partition可能放在不同的节点上，所以可能会有网络传输，但是没有涉及到数据的重新分区，只是两个RDD对应分区的合并，所以不需要重新起一个新的Stage，读取上一个Stage的结果，直接在当前的Stage中继续运行就好了。

co-partitioned的RDD能够大大减少由于Shuffle引起的网络传输
