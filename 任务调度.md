[原文链接](https://spark.apache.org/docs/latest/job-scheduling.html#dynamic-resource-allocation)
[参考](https://spark-reference-doc-cn.readthedocs.io/zh_CN/latest/more-guide/job-scheduling.html)
# 未完，还缺Scheduling Within an Application
## 概述
Spark有多种计算资源的调度工具。首先，每个Spark应用程序(一个SparkContext的实例对象)以进程的方式运行一组独立的Executors。Spark所使用的集群资源管理器提供了Spark应用程序之间的资源调度。其次，在每个Spark应用程序内部，行动操作提交的job可能在不同的线程中同事运行。如果你的应用程序服务于网络请求，那么这种情况会很常见。Spark在每个SparkContext(应用程序)内部提供公平调度策略来调度资源。

## 应用程序之间的资源调度
在集群上运行时，每个Spark应用程序都会有自己独立的一组Executor JVM，只负责该应用程序中任务的执行与数据的存储。如果多个用户共享一个集群，那么根据所使用的资源管理器的类型，会有很多不同的策略管理资源的分配。

最简单的资源分配选项，也是在所有资源管理器上都适用的，就是对资源进行静态划分。在这种静态划分方法下，一个应用程序分最多能使用多少资源，那么就给它分配多少资源，并且在整个应用程序运行期间，会锁定这些资源不释放。这也是Spark的Standalone、Yarn模式以及Mesos粗粒度模式默认所使用的资源分配方式。根据资源管理器类型的不同，资源分配可以如下配置：
- **Standalone模式**，默认情况下，提交的应用程序会按照FIFO顺序被处理，每个应用程序都会视图使用全部可用的节点(核数，cores)。可以对某个应用程序通过配置选项`spark.cores.max`，或修改所有应用程序的默认配置`spark.deploy.defaultCores`来限制一个应用程序所能使用的节点的数量。最后，除了对核数的控制外，还可以通过为每个应用程序配置`spark.executor.memory`来限制每个Executor所能使用的内存大小。
- **Mesos**，在Mesos上使用静态分配模式，需要设置`spark.mesos.coarse`为`true`，选择性的设置`spark.cores.max`来限制每个应用程序能够使用的最大核数(与Standalone模式一样)。另外，也应该设置`spark.executor.memory`来限制每个Executor所能使用的内存大小。
- **Yarn模式**，`--num-executors`选项用于在提交spark程序时指定要在集群上启动多少个Executor(配置选项`spark.executor.instances`是同样的作用)；`--executor-memory`(以及配置选项`spark.executor.memory`)和`--executor-cores`(以及配置选项`spark.executor.cores`)控制每个Executor能够使用到的核数以及内存大小。

在Mesos上，另一个可选的方式是动态的共享CPU的核数。这种模式下，每个Spark应用程序仍然有固定的、独立的内存分配(通过`spark.executor.memory`设置)，但是可以共享机器的CPU：当一个应用程序没有在一台机器上跑任务时，其他的应用程序可以占用这台机器的CPU。即，CPU的某个核不再是固定的分配给某个应用程序的一个Executor。这种方式在有大量不太活跃的应用程序时很有效，如不同用户的Spark shell。基于同样的原因，这种策略可能会有一定的延迟，因为当Spark应用需要使用CPU的时候，可能需要等待一段时间才能取得CPU的使用权。要使用这种模式，只需要在 `mesos:// URL` 上设置 `spark.mesos.coarse` 属性为flase即可。

注意，目前还没有任何一种资源分配模式能支持跨Spark应用的内存共享

### 动态资源分配
Spark提供一种机制，能够动态的根据工作负载调整应用程序所占用的资源大小。这意味着，当应用程序不需要某些资源时，会将他们还给资源管理器；而当需要更多的资源时，又会重新的发出请求。这一特性在多个应用程序共享同一Spark集群的资源时会很有用。

这一特性在默认情况下是关闭的。动态资源分配在所有的粗粒度集群资源管理器上都是可用的，如，Standalone模式、Yarn以及Mesos粗粒度模式。

#### 配置与部署
要使用这一特性需要两个条件：
- 配置`spark.dynamicAllocation.enabled`为`true`
- 在集群的每个工作节点上启动一个外部混洗服务(external shuffle service)，并设置`spark.shuffle.service.enabled`为`true`

外部混洗服务的目的是为了能够在不删除某个Executor写的shuffle文件的情况下，删除这个Executor。建立外部混洗服务的方法在不同的资源管理器上各有不同：
- Standalone模式，只需要设置`spark.shuffle.service.enabled`为`true`即可
- Mesos粗粒度模式下，在所有的slave节点上运行`$SPARK_HOME/sbin/start-mesos-shuffle-service.sh`，并设置`spark.shuffle.service.enabled`为`true`
- Yarn模式下，参照 [Running Spark on Yarn](https://spark.apache.org/docs/latest/running-on-yarn.html#configuring-the-external-shuffle-service)

#### 资源分配策略
总体上来说，Spark应在不需要时放弃对Executor的占有，并在需要时重新申请。由于没有一种明确的方式能用来预测一个将要被移除的Executor是否会在短暂的将来执行一个新的任务，或者一个新被添加的Executor是否会是处于idle的状态，因此我们需要一种启发式的方法，来决定何时申请或移除Executors。

##### 申请策略
开启了动态资源分配的Spark应用程序，会在当它有任务处于等待调度时申请额外的Executor。这种情况意味着，当前所拥有的Executors不足以同时运行所有被提交并且还没有被完成的task。

Spark会按轮次来申请Executors。实际的申请动作会首次在应用程序中等待调度的task持续存在超过`spark.dynamicAllocation.schedulerBacklogTimeout`所设置的秒数时发生，在这之后，如果应用程序的待调度任务队列中仍有等待调度的tasks，那么申请动作会每隔`spark.dynamicAllocation.sustainedSchedulerBacklogTimeout`秒执行一次。另外，每一轮中申请的Executor的数目成指数增长，比如，第一轮申请1个，接下来的几轮会依次申请2、4、8...等个。

Executor申请的数量设计为指数型增长的原因有两个：
1. 应用程序最开始申请Executor时应秉持谨慎的原则，以备实际上它只需要很少数量的Executor。与TCP的慢启动有些类似
2. 确保应用程序有能力及时增加它的资源使用，以防实际上它就是需要很多Executor

##### 移除策略
移除Executor的策略要简单很多。Spark应用程序中的一个Executor会在空闲超过`spark.dynamicAllocation.executorIdleTimeout`秒之后被移除。在大多数情况下，移除策略发生的条件与申请策略发生的条件是互斥的：如果应用程序中仍有等待调度的task，那么就不应该有空闲的Executor。

#### 优雅的回收Executor
在非动态分配中，Executor退出的原因有两个：执行失败，或spark应用程序退出。在任一中情况下，退出的Executor的状态都不再需要，因而可以安全的删除掉。而在动态分配中，是想要在一个spark应用程序运行的过程中移除掉它的一个Executor。因而，如果应用程序想要访问保存在该Executor中的状态或由该Executor写下的状态，由于Executor被移除了，应用程序就不得不重新计算一遍这个状态。为解决这个问题，Spark提供一种机制，能够在Executor被移除前，先保存下它的状态，以备后面访问。

这一点在shuffle的过程中尤其重要。在shuffle过程中，Executor首先会把自己的map输出存放到本地磁盘上的文件，然后Executor会充当server的角色供其他Executor访问这些文件。相同的任务会在RDD的不同partition上执行，当某些任务执行缓慢，用时远多于其他task时，动态分配策略就有可能在shuffle操作完成之前把这些已经计算结束、充当文件服务器、处于等待中的Executor回收掉。从而导致被回收的Executor上的混洗结果文件需要被重新计算一遍。

为解决这一问题，在Spark1.2中引入了外部混洗服务，用于保存shuffle文件。这一服务依赖于在集群的每个节点上长期运行的一个进程，这个进程独立于Spark应用程序以及它的Executors。外部混洗服务启用后，Spark的Executor不再直接从Executor中获取shuffle文件，转而从这个服务中获取。这样，当一个Executor被回收后，由这个Executor计算得到的shuffle结果仍然能够访问的到。

除了混洗文件以外，Executor也会在磁盘或内存中缓存数据。Executor被移除后，所有这些缓存的数据同样也无法访问了。所以，在默认情况下，带有缓存数据的Executor是永远不会被移除的。当然也可以手动配置一个时间期限(参数`spark.dynamicAllocation.cachedExecutorIdleTimeout`，默认是无限久)，超过期限这种Executor也会被回收。

## 应用程序内部的资源调度
