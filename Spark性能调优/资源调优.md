Spark(以及Yarn)使用的最主要的两个资源是CPU和内存。磁盘和网络IO当然也是Spark性能的一部分，但YARN和Spark目前都没有对这些资源进行有效的管理。

一个Spark应用程序中的每个Executor都由固定数量的cores和固定大小的堆内存。

- cores数量可以通过在提交程序(spark-submit)的时候使用`--executor-cores`来指定，也可以在配置文件`spark-defaults.conf`中通过配置选项`spark.executor.cores`来指定
- 堆内存，通过`--executor-memory`在提交程序时指定，也可以在配置文件中使用`spark.executor.memory`来指定

cores数目决定了在一个Executor中可以并行的执行task的数量。比如`--executor-cores 5`表示每个Executor同时最多可以执行5个task。内存影响了Spark可以缓存的数据的大小，以及shuffle数据的大小。

`--num-executors`命令行选项或配置参数`spark.executor.instances`控制一个SparkContext启动的executor的数量。(可以通过启动动态分配，来让Spark自动决定启用多少个executor)

相关的YARN的配置有：
- `yarn.nodemanager.resource.memory-mb`，单个节点上所有containers所能使用的最大内存
- `yarn.nodemanager.resource.cpu-vcores`，单个节点上所有containers所能使用的最多核数

Executor请求5个core会向YARN发出申请，请求5个虚拟的core。向YARN申请内存会比较复杂，原因有几点：
- `--executor-memory`或`spark.executor.memory`控制executor使用的堆内存，但是JVM同样会使用堆外内存(比如interned字符串或直接字节缓冲区)。由此加了另外一个参数`spark.yarn.executor.memoryOverhead`来估计堆外内存的大小。默认值是`max(384, 0.07*spark.executor.memory)`。因此实际申请的资源是堆内与堆外两部分内存相加
- YARN本身可能会把申请的内存自己调大一点。YARN的配置选项`yarn.scheduler.minimum-allocation-mb`和`yarn.scheduler.increment-allocation-mb`分别指定申请的资源的最小值和在原本申请的内存的基础上多给你分配多少内存

Spark on YARN的内存结构：
![内存分配](http://blog.cloudera.com/wp-content/uploads/2015/03/spark-tuning2-f1.png)

还有几点需要考虑的地方：
- Application Master所占用的资源也需要预先考虑到，因为AM的资源申请与Executor Container的自愿申请并不相同
- Executor的内存过大可能或导致过多的垃圾回收
-


The application master, which is a non-executor container with the special capability of requesting containers from YARN, takes up resources of its own that must be budgeted in. In yarn-client mode, it defaults to a 1024MB and one vcore. In yarn-cluster mode, the application master runs the driver, so it’s often useful to bolster its resources with the --driver-memory and --driver-cores properties.
Running executors with too much memory often results in excessive garbage collection delays. 64GB is a rough guess at a good upper limit for a single executor.

I’ve noticed that the HDFS client has trouble with tons of concurrent threads. A rough guess is that at most five tasks per executor can achieve full write throughput, so it’s good to keep the number of cores per executor below that number.
Running tiny executors (with a single core and just enough memory needed to run a single task, for example) throws away the benefits that come from running multiple tasks in a single JVM. For example, broadcast variables need to be replicated once on each executor, so many small executors will result in many more copies of the data.
