### spark日志管理

在`conf`目录下新建一个名为`log4j.properties`的文件，来管理日志设置。`conf`目录下已经有了一个日志设置文件的模板`log4j.properties.template`。默认设置是将所有信息都打印到控制台：
 
<pre>
log4j.rootCategory=INFO, console
</pre>

可以通过降低日志的级别（如降为只显示警告和更严重的信息）来减少日志：

<pre>
log4j.rootCategory=WARN, console
</pre>

### spark命令行使用

<pre>
spark-shell

val lines = sc.textFile("README.md")		// 创建了一个名为lines的RDD
lines.count()									// 统计RDD中的元素个数，也就是文本的行数
lines.first()									// RDD中的第一个元素，也就是文件中的第一行
</pre>

### Spark核心概念简介

从上层来看，每个spark应用都由一个**驱动器程序**来发起集群上的各种并行操作。驱动器程序包含main函数，并且定义了分布式数据集以及对数据集的操作。驱动器程序通过一个`SparkContext`对象来访问Spark，这个对象代表对集群的一个连接。

在命令行模式下，驱动器程序就是Spark Shell本身，并且Spark Shell在启动时会自动创建一个SparkContext类型的对象（sc）。

一旦有了SparkContext对象，就可以用它来创建RDD。RDD的数据可以分布在不同的节点上，为并行处理，一般也会有多个工作节点。即驱动程序一般要管理多个工作节点。如下图所示。

### SparkContext的初始化

SparkContext对象需要一个SparkConf对象来完成创建：

<pre>
val conf = new SparkConf().setMaster("local").setAppName("test")
val sc = new SparkContext(conf)
</pre>

创建SparkContext的最基本的方法，需要传递两个参数：

* 集群URL，告诉Spark如何连接到集群上。`local[n]`表示让Spark在单机n线程上运行任务。
* 应用名，
