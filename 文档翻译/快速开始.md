# 快速开始

这篇教程会简单介绍Spark的使用。我们首先通过Spark的交互式命令行(Scala或Python)介绍API，然后介绍如何使用Java、Scala或Python编写应用程序。

遵循本指南，首先需要到Spark官网下载一个发行版的包。由于我们不会使用HDFS，因此可以下载为任意Hadoop版本编译的Spark版本。

注意，Spark2.0之前，Spark的主要编程接口是RDD。Spark2.0之后，RDD由DataSet取代，与RDD一样都是强类型的，但是底层提供了更丰富的优化。Spark目前仍然支持RDD接口，并且可以从RDD编程指南中得到更详细的参考信息。但是强烈建议切换到DataSet接口使用，因为它的性能要比RDD好很多。查看SQL编程指南来了解有关DataSet的更多信息。

## 使用Spark-Shell进行交互式分析
### 基础
Spark Shell提供了一种学习Spark API的简单途径，以及交互的分析数据的强力的工具。Spark Shell支持使用Scala(运行在Java虚拟机上，从而可以很方便的使用现有的Java库)和Python。在Spark目录下通过如下命令启动Spark Shell：
```Scala
./bin/spark-shell
```
Spark最主要的一个抽象概念是一个称作DataSet的分布式数据集。DataSet可以由Hadoop兼容的输入格式(如HDFS文件)创建，或从其他DataSet转换而来。让我们使用Spark目录下的README文本文件来创建一个新的DataSet：
```
scala> val textFile = spark.read.textFile("README.md")
textFile: org.apache.spark.sql.Dataset[String] = [value: string]
```
你可以通过调用行动操作，来从DataSet中直接获取到值，也可以通过转化操作将DataSet转化为一个新的DataSet。更详细信息可以查看[API文档](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset)。
```
scala> textFile.count() // Number of items in this Dataset
res0: Long = 126 // May be different from yours as README.md will change over time, similar to other outputs

scala> textFile.first() // First item in this Dataset
res1: String = # Apache Spark
```
现在我们将它转化为一个新的DataSet。可以调用`filter`方法得到一个包含原有文件中数据的子集的新的DataSet。
```
scala> val linesWithSpark = textFile.filter(line => line.contains("Spark"))
linesWithSpark: org.apache.spark.sql.Dataset[String] = [value: string]
```
可以将转换操作与行动操作串联在一起：
```
scala> textFile.filter(line => line.contains("Spark")).count() // How many lines contain "Spark"?
res3: Long = 15
```

### DataSet上的更多操作
DataSet的行动操作和转换操作可以用在更复杂的计算中。比如，查找单词数最多的那一行：
```
scala> textFile.map(line => line.split(" ").size).reduce((a, b) => if (a > b) a else b)
res4: Long = 15
```
首先使用`map`函数将每一行转换为一个整数，从而得到了一个新的DataSet。在新的DataSet上调用`reduce`函数来找到最大的那个数字。`map`函数和`reduce`函数的参数是Scala中的函数字面值，可以任意使用的语言特性或Scala/Java库。比如我们可以很方便的调用其他地方声明的函数。我们使用`Math.max()`函数来使得这段代码更容易理解：
```
scala> import java.lang.Math
import java.lang.Math

scala> textFile.map(line => line.split(" ").size).reduce((a, b) => Math.max(a, b))
res5: Int = 15
```
一个常用的数据流模式是MapReduce，Spark可以很简单的实现这一流程：
```
scala> val wordCounts = textFile.flatMap(line => line.split(" ")).groupByKey(identity).count()
wordCounts: org.apache.spark.sql.Dataset[(String, Long)] = [value: string, count(1): bigint]
```
这里，首先使用`flatMap`将包含一行行数据的DataSet转换为包含一个个单词数据的DataSet。然后调用`groupByKey`和`count`函数，计算文件中每个单词的出现次数，结果是元素类型`(String, Long)`的DataSet。使用`collect`函数获取最终的结果：
```
scala> wordCounts.collect()
res6: Array[(String, Int)] = Array((means,1), (under,2), (this,3), (Because,1), (Python,2), (agree,1), (cluster.,1), ...)
```
### 缓存
Spark还支持将数据拉到集群范围内的内存中缓存。在数据被频繁的访问的情况下，这会很有用。比如访问一个小的热数据，或者运行一个像PageRank类似的迭代算法。作为例子，我们把`linesWithSpark`这个DataSet缓存起来：
```
scala> linesWithSpark.cache()
res7: linesWithSpark.type = [value: string]

scala> linesWithSpark.count()
res8: Long = 15

scala> linesWithSpark.count()
res9: Long = 15
```
使用Spark访问并缓存一个100行数据的文本文件本身可能有点蠢。但有趣的点在于这些函数可以被用到很大的数据集上面，即使它们分布在成百上千个节点上。上面的spark-shell是运行在本机节点行的，你也可以将spark shell运行在一个cluster上，详细的在[RDD编程指南](https://spark-reference-doc-cn.readthedocs.io/zh_CN/latest/programming-guide/rdd-guide.html#rdd-programming-guide)中介绍。

## 自包含的应用程序
假设我们想用Spark API写一个自包含的应用程序，我们使用Scala(sbt管理)来完成一个简单的自包含应用程序。
在`SimpleApp.scala`文件中编写如下内容：
```
/* SimpleApp.scala */
import org.apache.spark.sql.SparkSession

object SimpleApp {
  def main(args: Array[String]) {
    val logFile = "YOUR_SPARK_HOME/README.md" // Should be some file on your system
    val spark = SparkSession.builder.appName("Simple Application").getOrCreate()
    val logData = spark.read.textFile(logFile).cache()
    val numAs = logData.filter(line => line.contains("a")).count()
    val numBs = logData.filter(line => line.contains("b")).count()
    println(s"Lines with a: $numAs, Lines with b: $numBs")
    spark.stop()
  }
}
```
应用程序需要定义一个`main`函数，而不能继承自`scala.APP`(因为`APP`的子类可能会不正常工作)。

这个程序仅仅分别计数了包括a和包括b的行的数目。注意，需要替换其中的`YOUR_SPARK_HOME`为你本地实际的Spark安装路径。与Spark Shell不同，Spark Shell会自动初始化一个`SparkSession`，在自包含程序中，我们需要自己创建一个`SparkSession`。

`SparkSession`的创建需要调用`SparkSession.builder`函数，然后指定应用程序的名称，最后调用`getOrCreate`函数来得到这个`SparkSession`的实例。

我们的程序依赖于Spark API，因此需要在sbt文件中添加Spark依赖，如：
```
name := "Simple Project"

version := "1.0"

scalaVersion := "2.11.12"

libraryDependencies += "org.apache.spark" %% "spark-sql" % "2.4.0"
```
为了让sbt能正常工作，我们需要把这两个文件按照一般的Scala目录结构来存放。就绪后，就可以编译得到一个Jar包。可以使用Spark提供的`spark-submit`脚本来提交Jar包，运行程序。

## 下一步
- [RDD编程指南](https://spark.apache.org/docs/latest/rdd-programming-guide.html)
- [Spark SQL编程指南](https://spark.apache.org/docs/latest/sql-programming-guide.html)
- [集群上部署](https://spark.apache.org/docs/latest/cluster-overview.html)
