## Spark Streaming状态管理
在流式数据中，我们有时需要维护某个状态。比如我司要统计某个用户的行为，打点数据不断的过来，我们需要知道用户何时开始一个新的session，session中做了什么，session什么时候结束等。这就要求我们维护一个session的状态，并能够在一段时间内没有用户数据时自动结束掉session。

Spark Streaming为状态管理提供了两种方法：
1. **updateStateByKey**
  Spark Streaming是基于微批进行处理的，每个micro-batch都会形成一个RDD传入Spark进行处理。因此，最直接的方法就是：当前batch形成的RDD与之前的计算结果RDD进行聚合，产生一个新的RDD。这种方法很直接，但是效率较低，因为每个批次处理结束后都需要与之前维护的状态RDD完成聚合操作，在数据量不断增大的情况下，效率会越来越低。
2. **mapWithState**
  由于RDD是不可变的数据集合，因此在Spark1.6之后提供了`MapWithStateRDDRecord`这一可变集合，并放到`MapWithStateRDD`中。用`MapWithStateRDDRecord`维护所有key的状态。

下面介绍具体的使用方法

### updateStateByKey

// TODO 没用过

### mapWithState

【**作用**】
将用户定义的function作用在流中的每个key-value元素对上，并为每个key维护某种状态，返回类型为`[[MapWithStateDStream]]`。
```Scala
def mapWithState[StateType: ClassTag, MappedType: ClassTag](
      spec: StateSpec[K, V, StateType, MappedType]
    ): MapWithStateDStream[K, V, StateType, MappedType]
```
【**基本使用**】
接收一个`StateSpec`类型，这个`StateSpec`类型的作用是将提供的function进行包裹。比如一个WordCount计数的程序：
```Scala
val stateSpec = StateSpec.function {
  (key: String, value: Option[Int], state: State[Int]) =>
    val sum = value.getOrElse(0) + state.getOption.getOrElse(0)
    state.update(sum)
    (word, sum)
}
```
对比上面的作用介绍，key是key-value对中的键，value是key-value对中的值，state是我们想要为key维护的对应到状态使用`State`进行的包裹。整个function的作用就是基于当前传过来的key-value对，对维护的状态进行更新。

【**其他设置**】
`StateSpec`封装了用户逻辑(function)，还提供了其他的设置，如初始状态、超时时间：

```Scala
// 代码参考源码中JavaStatefulNetworkWordCount
val initialRDD = ssc.sparkContext.parallelize(List(("hello", 1), ("world", 1)))
val stateSpec =
  StateSpec
    .function {
      (key: String, value: Option[Int], state: State[Int]) =>
      val sum = value.getOrElse(0) + state.getOption.getOrElse(0)
      stte.update(sum)
      (word, sum)
    }
    .timeout(Seconds(60))
    .initialState(initialRDD)
```
超时时间是指：如果一段时间内没有收到某个key的数据，则停止对该key状态的维护并删除。在删除后如果重新收到该key的数据，会重新生成一个新的state，并进行维护。超时时间可以用来维护session的状态，超过一定时间将session自动结束。但这个时间有时候会很不准。

初始状态是指：在处理流数据之前，状态的初始值。

【**源码分析**】
// TODO
