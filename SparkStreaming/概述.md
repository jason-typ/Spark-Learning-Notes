SparkStreaming是在SparkCore的基础上进行的扩展，使用离散化流(Discretized Stream)来作为抽象表示，叫做DStream。DStream是随着时间推移而收到的数据的序列。SparkStreaming中的数据流向如下图。SparkStreaming可以接收多种源的数据，包括Kafka、Flume、Kinesis及socket等。数据在Spark内部经过处理后，可以写到文件系统、数据库等地方。
![SparkStreaming数据流向](images/2019/02/sparkstreaming数据流向.png)

在SparkStreaming内部，每个时间区间内收到的数据都作为RDD存在，作为SparkStreaming流的抽象表示DStream，就是由这些RDD组成的序列。所以SparkStreaming并不是真正的流式处理，而是基于SparkCore的“微批”操作。如下图：
![微批操作](images/2019/02/微批操作.png)

### 一个简单的例子

```Scala
object SparkStreaming {
  def main(args: Array[String]): Unit = {
    val conf = new SparkConf().setMaster("local[2]").setAppName("SparkStreaming")
    val ssc = new StreamingContext(conf, Seconds(1))
    val lines = ssc.socketTextStream("localhost", 9999)
    val words = lines.flatMap(_.split(" "))
    val pairs = words.map(word => (word, 1))
    val wordCounts = pairs.reduceByKey(_ + _)

    // Print the first ten elements of each RDD generated in this DStream to the console
    wordCounts.print()

    ssc.start()
    ssc.awaitTermination()
  }
}
```

和批处理相同，都需要建立一个运行环境，在SparkStreaming中叫做`StreamingContext`，这是程序的入口(在旧版RDD中是SparkContext，在SparkSQL中是SparkSession)。由于SparkStreaming是将一段时间间隔内的流数据当做批来处理，因此需要设置一个时间间隔(上面是1s)。接着创建了一个监听本地端口的流，并在这个流上执行一系列操作。上面的代码定义了计算过程，并设置每秒触发。要开始接收数据，需要显式的调用`StreamingContext`的`start`方法，这样SparkStreaming就会把作业不断交给`StreamingContext`下面的`SparkContext`去调度。任务的执行会在另一个线程中，所以需要调用`awaitTermination`来等待流计算完成。

### 架构与抽象
离散化数据流(Discretized Stream)或DStream是SparkStreaming提供的最基本的抽象。它代表了一个连续的输入流。在Spark内部，每个时间区间内收到的数据都作为RDD存在，DStream就是由这些RDD组成的序列。
![DStream](https://spark.apache.org/docs/latest/img/streaming-dstream.png)

任何对DStream的操作都会转化为对RDD的操作。因此SparkStreaming实际上是将流当做微批来处理，微批包括一个可设置的时间间隔内的所有的数据。

Spark Streaming在Spark的驱动器程序-工作节点结构下的执行过程如下图。Spark Streaming为每个输入源启动对应的接收器。接收器以任务的形式运行在应用的执行器进程中，从输入源收集数据并保存为RDD。它们收集到数据后会把数据复制到另一个执行器进程以保证容错性。数据保存在执行器的内存中(也可以设置保存在HDFS等上)。驱动器程序中的StreamingContext会周期性的运行Spark作业来处理这些数据，并把数据与之前时间区间内的RDD进行整合。

### 容错性
Spark Streaming为DStream提供的容错性与Spark为RDD提供的容错性一致：只要输入数据还在，就可以使用RDD的谱系重算处任意状态。

默认情况下，收到的数据分存在两个节点上，因此可以允许任意一个节点失效。不过只使用谱系图来恢复效率有点低，因此Spark Streaming提供了检查点机制：可以把状态阶段性的存放到可靠的文件系统中。一般来说可以处理5-10个批次，就保存一次状态。这样，在恢复数据时，Spark Streaming只需要回到上一个检查点即可。

### 输入源
SparkStreaming提供了两大类数据源的支持：
- 基础源：从`StreamContext`的API中可以直接获取到的源，如文件系统、socket连接等
- 高级源：如Kafka、Flume、Kinesis。这些源需要添加其他的依赖

需要注意的是，任何一个DStream(除了FileStream)都需要关联一个接收者。接收者负责读取数据，放在内存中，留待数据处理程序来处理。因此，在local模式下运行SparkStreaming时，至少需要两个线程：一个用来接收数据，一个用来处理数据。

##### 1. Basic Sources
##### 1.1 File Streams
从任何与HDFS API兼容的文件系统上读取文件，可以通过`StreamingContext.fileStream[KeyClass, ValueClass, InputFormatClass]`创建一个DStream。File Stream不需要一个receiver，因此不需要为接收数据单独分配一个核或线程。

```Scala
streamingContext.fileStream[KeyClass, ValueClass, InputFormatClass](dataDirectory)
```
对简单的文本文件，可以简单的通过调用`streamingContext.textFileStream(dataDirectory)`来创建DStream。

```Scala
streamingContext.textFileStream(dataDirectory)
```

###### 监控目录
SparkStreaming会监控参数中指定的目录，对该目录下创建的任何文件都进行处理。


### DStream操作
DStream的操作分为两种：转化操作和输出操作。分别类似于RDD的转化操作和行动操作。
##### 转化操作
DStream的
##### 输出操作
输出操作和RDD的行动类似，Spark Streaming的输出操作在每个时间区间内周期的执行，每个批次都生成输出。
