# 数据读取与保存
Spark支持多种数据源，三种常见的数据源：

- 文件格式与文件系统
  Spark支持本地文件系统以及分布式文件系统(HDFS、S3)中的多种文件格式，包括文本文件、JSON、SequenceFile以及ProtocolBuffer等。
- Spark SQL中的结构化数据源
- 数据库与键值存储

## 文件系统
Spark支持本地文件系统以及分布式文件系统(HDFS、S3)。

- 对于本地文件系统，Spark要求文件在集群中所有节点的相同目录下都能找到，因为task最终是在Executors中执行的。本地文件系统使用协议`file`，如`sc.textFile("file:///usr/local/spark/README.md")`
- 对于HDFS，使用hdfs协议，需要指定master的地址和端口，如`sc.textFile("hdfs://masterIP:Port/path")`

## 文件类型
Spark会根据文件的扩展名，选择对应的处理方式。

##### 1. 文本文件
将一个文本文件读取为RDD时，文本文件中的每一行都会变成RDD的一个元素。也可以将多个文本文件或目录读取为PairRDD，其中键是文件名，值是文件内容。

读取文本文件使用`textFile`方法，需要传入文件的路径，如：
```Scala
sc.textFile("file:///usr/local/spark/README.rd")
```
如果传入的参数是个目录的路径，`textFile`函数会把所有文件的内容组合到一起作为一个RDD返回。传递目录路径，也可以使用另一个函数`SparkContext.wholeTextFiles()`，这个函数会返回PairRDD，其中键是文件名，值是文件内容。另外，Spark支持路径中的通配符，如`part-*.txt`。

保存文本文件使用`saveAsTextfile`。Spark会将参数当做目录对待，输出放在目录下的多个文件中。RDD有几个分区就会把RDD的内容输出到几个文件中，使用并行加快执行速度。

##### 2. JSON
读取JSON格式数据最简单的方式是将JSON文件作为文本文件读入，然后使用JSON解析器进行解析。这种方法假设文件中的每一行都是一条JSON数据。如果JSON数据有分行，则需要使用`wholeTextFile`方法将整个文件的内容作为RDD的一个元素进行读取，然后对文件进行解析。

JSON文件的写很方便：将代码中的数据转换为JSON格式的数据，以文本格式输出就好。

##### 3. 逗号分隔值和制表符分隔值
各个字段之间使用逗号或者制表符分隔开

##### 4. SequenceFile
##### 5. HBase


## 数据读取
Spark支持多种数据源

- textFile(URL)
  - 由于工作的执行是在Executors上，因此读取数据需要保证每个Executor都能够访问到这个URL，特别是对于本地文件系统来说
  - 可以添加一个可选的参数，指定RDD的分区数量。默认情况下，对分布式文件系统来说，Spark会为文件的每个Block创建一个Partition。
  - 这个方法把文件的每一行当做RDD的一个元素返回
- SparkContext.wholeTextFiles
  - 读取包含多个文件的目录，返回PairRDD，key是文件名，value是文件的内容
- sequenceFile[K, V]，用于读取SequenceFile，K和V分别是文件中的key和value的类型。这些类型需要是Hadoop中`Writable`接口的子类。
- SparkContext.hadoopRDD
  -
