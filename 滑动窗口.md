### 1. 滑动窗口
最早接触滑动窗口是在TCP协议中：为了防止Receiver的缓冲区被Sender一下子撑爆，Receiver需要告知到Sender自己还能接收多少数据。在之前的工作中，也有过类似的一个应用。前端在读取一个视频文件时，由后端逻辑决定发送什么数据，前端插件读取到结束符后才会停止。为了防止后端一下子发送过多数据把插件的缓冲区撑爆，就做了这么一个类似的滑动窗口，不断的给前端发送数据。

在Streaming程序中的滑动窗口跟前面说的从目的上是不一样的：Streaming程序中是为了聚合一段时间内的数据。如下图所示。

![Spark-Streaming-Window-Operation](https://spark.apache.org/docs/latest/img/streaming-dstream-window.png)

流式处理程序中的Window代表的是一个固定大小Interval内的数据。有两个属性：
- **窗口大小**，又或者说时间间隔
- **滑动间隔**，很自然的联想到了奈奎斯特采样定律，一个Data Flow看起来就像是一个信号，画出来就是一个波形。滑动间隔就定义了多久对这个数据流执行一次采样

如上图中，加入time间隔是1s，即1s产生一个batch的数据，那么窗口大小就是3s，包含3个连续的batch的数据；滑动间隔是2s，每2s采一次样。从上面描述可以看出，这两个参数，包括窗口大小和滑动间隔，都必须是batch interval的整数倍。

### 2. SparkStreaming中滑动窗口支持的操作
Window操作，无论具体是什么，都是对一段数据内的操作，把整个DataFlow给延长，window操作的结果不断的形成，也是一个不断延伸的DataFlow，或者说DStream。这个Flow中的每个数据都是original Flow中一段数据的计算结果。下面只是将官网上常用的一些window操作搬了过来

- window(windowLength, slideInterval)
  根据给定的窗口大小和滑动间隔，返回一个新的DStream，即上图中的windowed DStream
- countByWindow(windowLength, slideInterval)
  得到windowed DStream中每个window中数据个数
- reduceByWindow(func, windowLength, slideInterval)
  对每个window内的数据做聚合操作
- reduceByKeyAndWindow(func, windowLength, slideInterval, [numTasks])
  针对key-value形式的DStream：对每个window内的key-value形式的数据做聚合操作
- reduceByKeyAndWindow(func, invFunc, windowLength, slideInterval, [numTasks])
  与上一个函数类似，但是之前的函数计算的都是每个window内的数据，各个window之间没有联系。这个函数对每个window内的key-value做聚合后，会根据之前window的聚合结果，更新本次的聚合结果。最简单的，例如累加
- countByValueAndWindow(windowLength, slideInterval, [numTasks])
  同样针对key-value形式的数据，对每个window内的key-value对的key进行频次统计

关于具体使用，可以查看这篇[文章](https://www.cnblogs.com/duanxz/p/4408789.html)

### 3. StructuredStreaming
StructuredStreaming中滑动窗口的操作是相同的，结果也是相同的，不过可能实现的方式有所不同。在StructuredStreaming的window操作中，类似于一张可以更新的表，是为每个时间窗口在表中维护了一条(或多条)数据，如：
![Structured-Streaming-Window-Operation](https://spark.apache.org/docs/latest/img/structured-streaming-window.png)

对于Window操作来说，表的第一级结构就是windowLength。在图中，窗口大小是10min，滑动间隔是5min，那么对于12:10这个时间点来说，会影响到的时间窗口有：12:00-12:10和12:05-12:15。因此在12:10这个时间点上，会更新这两个时间窗口。当然，对于12:05-12:15这个时间窗口，数据还没有全部处理，12:10-12:15这个窗口的数据还没有到来，所以在12:15这个时间点上，还会更新12:05-12:15这个窗口的数据。
