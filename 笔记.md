
### Schema
Schema定义了列的名称以及数据类型。在Spark内部，Schema使用StructType来表示。StructType中包含多个StructField。StructField定义为：

```
case class StructField(name: String, dataType: DataType, nullable: Bolean, metadata: Metadata)
```

Spark中Schema可以自动推断，也可以显式的指定，在生产环境中一般需要指定。在代码中，如何为某个DataFrame创建并指定一个Schema：

```
val myManualSchema = StructType(Array(
  StructField("DEST_COUNTRY_NAME", StringType, true),
  StructField("ORIGIN_COUNTRY_NAME", StringType, true),
  StructField("count", LongType, false, Metadata.fromJson(s"""{"hello": "world"}"""))
))
val dataFrame = spark.read
  .format("json")
  .schema(myManualSchema)
  .load("file:///Users/tang/Documents/data/flight-data/json/2015-summary.json")
```

Schema定义了三列(三个StructField)，在定义DataFrame时使用`schema`函数传入自定义的Schema。运行时，如果数据不符合预定义的Schema，Spark会抛出异常。

### Column与Expressions

访问DataFrame中的某个Column，可以使用`col`或`column`方法：

```
dataFrame.col("DEST_COUNTRY_NAME")
```
Spark中Columns类似于表格中的列，选择与修改这些列的操作叫做expressions。

Spark中，Columns就是Expressions，这句话怎么理解呢？首先看看Expression的定义：Expression是对DataFrame中一个record中的一个或多个值上的一系列transformation操作。可以把Expression理解为一个接收一个或多个Column名称的函数，Expression会解析这些名字，并对DataFrame中的每个Record都按照定义的transformation操作生成一个single value(可以是复杂类型)。

再查看Column的定义
```
class Column(val expr: Expression)
```
这样看可能就比较清楚了，比如最简单的可以访问DataFrame中的某个列，可以这样：

```
val column1 = col("count")
logger.error(dataFrame.select(column1).collectAsList().toString)
```

也可以做一些其他操作，比如减去一个数：
```
val column2 = dataFrame.col("count") - 5
logger.error(dataFrame.select(column2).collectAsList().toString)
```

也可以这样：
```
val ret = dataFrame.where(dataFrame.col("count") > 20)
logger.error(ret.collectAsList().toString())
```

所以说，Column其实就是一个生成器，会一个一个作用在DataFrame中的每个Record上，并生成一个Single Value。当然上面的Single Value都是简单类型(`Int`和`Boolean`)，还可以支持复杂一些的类型。现在应该能理解Columns are just expressions这句话。那么上面的代码完全可以改写为：
```
val column1 = col("count")
val expr1 = expr("count")
val column1Ret = dataFrame.select(column1).orderBy("count", "ORIGIN_COUNTRY_NAME").collectAsList().toString
val expr1Ret = dataFrame.select(expr1).orderBy("count", "ORIGIN_COUNTRY_NAME").collectAsList().toString
assert(column1Ret == expr1Ret, "Assert fail")

val column2 = col("count") - 5
val expr2 = expr("count") - 5
val column2Ret = dataFrame.select(column2).orderBy("count", "ORIGIN_COUNTRY_NAME").collectAsList().toString
val expr2Ret = dataFrame.select(column2).orderBy("count", "ORIGIN_COUNTRY_NAME").collectAsList().toString
assert(column2Ret.contentEquals(expr2Ret), "Assert2 fail")

val column3Ret = dataFrame.where(col("count") > 2).orderBy("count", "ORIGIN_COUNTRY_NAME").collectAsList().toString
val expr3Ret = dataFrame.where(expr("count") > 2).orderBy("count", "ORIGIN_COUNTRY_NAME").collectAsList().toString
assert(column3Ret.contentEquals(expr3Ret), "Assert2 fail")
```

这种写法很简单，但看起来好像比较奇怪，比如为什么`col("count") - 5`与`expr("count") - 5`以及`expr("count - 5")`都是等价的？原因是他们都是Column类型，Spark在编译的时候会通过一个逻辑树决定所有这些操作的顺序，在此基础上进行优化。

这也是为什么Spark中用各种不同语言编译得到的代码最终执行效率一样的原因，因为都是转换成了相同的Spark内部代码执行。


另外，如果想要直接明确的得到DataFrame中的某一列，可以直接使用`col`或`column`方法：
```
val columnCount = dataFrame.col("count")
```

###### Column其他操作
- 打印一个DataFrame的Schema，`dataFrame.printSchema()`
- 获取一个DataFrame的所有Column，`val columns: Array[String] = dataFrame.columns`


### Records与Rows
DataFrame就像一张表，一行就是一条记录(Record)，Spark中用`Row`来表示这一条记录。`Row`在Spark内部是一个byte数组，但上层是感知不到的，因为对`Row`的操作都是使用上面介绍的column expression来进行的。

要注意的是，只有DataFrame(不区分DataFrame与DataSet)才会有schema，而Row是没有schema的。因此在创建一个Row时，必须根据DataFrame中定义的schema按照顺序指定value，如：

```
import org.apache.spark.sql.Row
val myRow = Row("Hello", null, 1, false)
```

前面说过，`Row`在内部的存储就是一个字符数组，因此要访问某个元素，除了使用下标访问外，还需要使用类型强转：

```
val row = dataFrame.first()
row(0)                        // type Any, 或使用row.get(0)
row(0).asInstanceOf[String]     // 转为String
row.getString(0)                // 提供的便捷方法，直接获取位置0位置的元素，并转换为String
row.get(0).asInstanceOf[String] // 等价
```

## DataFrame操作

##### 1. select与selectExpr方法
这两个方法与SQL中的query语句类似。`select`方法需要传入列的名称(可以一次性传入多个)。

```
val dataFrame = readFromJson(spark, myManualSchema)
dataFrame.show(10)
dataFrame.select(myManualSchema.head.name, myManualSchema(2).name).show(10)
```

查看`select`的多个版本实现，发现最终都是调用的参数为`Column`的一个`select`，所以这里也可以直接传入`Column`参数。前面介绍了`Column`就是Expression，因此这里又可以有多种写法：
```
dataFrame.select(
  col(schema.head.name),
  expr(schema(1).name),
  dataFrame.col(schema(2).name))
```

但要注意一个常见的错误是把`Column`参数与`String`参数**混用**：`select`接收`String`或者`Column`，但不接受两者的混合。下面的代码就会报错：
```
df.select(col("DEST_COUNTRY_NAME"), "DEST_COUNTRY_NAME")
```

书上推荐使用`expr`这个方法，它提供的灵活性最好。前面说过，在传入`select`方法中时，下面这几个是等价的：
```
col("count") - 5
expr("count") - 5
expr("count - 5")
```
但是要明白他们的效果一致，实现方式不一致。

- `col`方法接收一个`String`，返回一个`Column`，后面的`-`是利用Scala的特性，将`-`这个符号定义成了一个方法，所以`col("count") - 5`与`col("count").-(5)`是等价的，即在Column对象上调用已定义的方法来实现。这与第二种方法的实现是一致的
- `expr`方法也接受一个`String`，但是内部会使用`SparkSqlParser`来解析这个String。也就是说其实是把这个字符串当做了sql来解析了，因此这里的减号就是SQL里的减号。

这个区别就决定了`expr`这个方法的灵活性是最好的：可以传入SQL语句片段。`col`方法需要依赖于`Column`这个类中已经实现的方法，况且SQL中的方法并没有完全实现。比如一个简单的例子，选取某一列并重命名，两个方法的实现分别为：
```
dataFrame.select(expr(s"""${schema.head.name} AS destination""")).show(2)
dataFrame.select(col(schema.head.name) as "destination").show(2)
```
使用`expr`可以直接写SQL，而使用`col`需要调用另外的已定义的方法。而如果要完成计数count这种工作，`Column`好像并没有实现这种方法，那么就只能使用`expr`(通过SQL解析器)来完成了：

```
dataFrame.select(expr("DEST_COUNTRY_NAME"), expr("ORIGIN_COUNTRY_NAME AS from"), expr("count")).printSchema()
```

实际上，由于这种使用方式太多：select后面跟expr，因此Spark提供了另一个方法`selectExpr`来缩减一点点代码。而这也是Spark的强大之处：可以使用SQL写复杂的表达式，完成DataFrame的转化操作。
```
dataFrame.selectExpr("avg(count)", s"""count(distinct(${schema.head.name}))""").show()
dataFrame.selectExpr("*", "(DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME) as withinCountry").show(10)
```



可以替代其他的方法用来指代一个column，也可以做一些简单的字符串操作，如AS，而使用其他方法会报错。
```
dataFrame.select(expr(s"""${schema.head.name} AS destination""")).show(2)
```

##### 2. Adding Columns(增加Columns)
调用DataFrame的`withColumn`方法完成，接收两个参数：String(列的名称)和`Column`(expression)。
```
dataFrame.withColumn("numberOne", lit(1)).show(10)
dataFrame.withColumn("withinOneCountry", expr("ORIGIN_COUNTRY_NAME == DEST_COUNTRY_NAME")).show(10)
```
`expr`函数前面了解过了，解析SQL得到一个`Column`(Expression)，这个`lit`是什么？lit是literal的简写，用于将各种编程语言使用的类型统一转换为Spark内部使用的类型。这个函数的返回值也是`Column`。

##### 3. Renaming Columns
前面介绍过方法可以rename一个column：使用as。如：
```
dataFrame.select(expr("DEST_COUNTRY_NAME, ORIGIN_COUNTRY_NAME AS from, count")).printSchema()
```
但这样写很麻烦，特别是列多的情况下。因此提供了一个便利的函数`withColumnRenamed`：
```
dataFrame.withColumnRenamed("ORIGIN_COUNTRY_NAME", "from").printSchema()
```

##### 4. Dropping Columns
Spark中使用不可变集合的概念，因此不会真的丢弃，只是生成了一个新的不带drop掉的那一列的新的DataFrame。所以，drop操作实际上是通过`select`完成的：select不需要被drop的列。但一直这样写会比较麻烦，比如drop的列很少，保留的列很多的情况下。因此提供了一个函数`drop`来完成这个工作，其实内部也是调用的`select`：
```
dataFrame.drop("DEST_COUNTRY_NAME").printSchema()
```

##### 5. 强制类型转换
有时需要将某个列的类型转化为另一个类型，如上面Schema定义中的count从long转换为int：
```
dataFrame.withColumn("count2", col("count").cast("int")).printSchema()
```

注意这里的`withColumn`方法，前面介绍的，这个方法是用来增加一个列的，因此方法的第一个参数就是新的列的名称。在方法内部也会检查是否与已经存在的列重名。另外，由于这个方法是用来增加列的，因此原本的列并不会被丢弃，如上面语句，结果就是使用count列根据定义的Expression计算结果来增加了一列，所以结果为：
```
|-- DEST_COUNTRY_NAME: string (nullable = true)
|-- ORIGIN_COUNTRY_NAME: string (nullable = true)
|-- count: long (nullable = true)
|-- count2: integer (nullable = true)
```

##### 6. Filtering Rows
过滤rows，使用`where`或`filter`函数，两者是等价的。都需要传入一个结果为布尔值的expression。如：
```scala
dataFrame.where("count > 10").where("""ORIGIN_COUNTRY_NAME != "Croatia"""").show(10)
```
注意，直接传入的字符串会被解析为列名称，因此需要使用引号括起来。`where`函数同样接收`String`或是`Column`，接收`String`会使用SQL解释器解析为`Column`。与`expr`的使用类似。

另外，由于Spark会对执行过程进行优化，所以不需要将多个限制条件放到一起或考虑先后性的问题，Spark会做这件事情，我们只需要把限制条件使用`where`或`filter`串在一起就可以了。

##### 7. Getting Unique Rows
去重功能可以使用`distinct`函数，如：
```scala
val df = dataFrame.select("ORIGIN_COUNTRY_NAME", "DEST_COUNTRY_NAME").distinct()
df.printSchema()
logger.error(df.count().toString)
```
与之等价的SQL语句为：
```sql
SELECT COUNT(DISTINCT(ORIGIN_COUNTRY_NAME, DEST_COUNTRY_NAME)) FROM dfTable;
```

##### 8. Random Samples
如果需要取DataFrame中的一部分查看数据，可以使用`sample`函数：
```
dataFrame.sample(0.2).count()
```
提供的参数指定取sample的比例(非严格)。另外还可以指定一个用于生成随机数的种子。

##### 9. Random Splits
如果需要将一个DataFrame按一定比例随机拆分为两个DataFrame，可以使用`randomSplit`:
```Scala
val dfs = dataFrame.randomSplit(Array(0.1, 0.2, 0.7))
for (df <- dfs) {
  logger.error(df.count.toString)
}
```

##### 10. Appending Rows
同创建新的Column一样，都是通过创建新的DataFrame来完成的，因为DataFrame是不变的。Appending操作通过`union`函数连接两个DataFrame来完成。注意要上层保证两个DataFrame具有相同的Schema(实际上不是通过Schema来完成的连接，而是通过位置信息)。
```Scala
val rows = Seq(
  Row("heihei", "other1", 5L),
  Row("hehe", "other2", 1L)
)
val rdd = spark.sparkContext.parallelize(rows)
val df = spark.createDataFrame(rdd, dataFrame.schema)
dataFrame.union(df).where(s"""count = 1""").where("""ORIGIN_COUNTRY_NAME != "United States" """).show()
```

##### 11. 排序
排序使用`sort`或者`orderBy`都可以，两者是等价的。都接收`String`或者`Column`类型。

##### 12. Limit
限制返回的个数

##### 13. Repartition和Coalesce
使用`repartition`函数重新分区，并且一定会涉及到shuffle操作，至少需要提供重新分区的个数。

`coalesce`函数用于减少分区的个数，如果提供的分区数比DataFrame当前的分区数要大，那么不会做什么改变。由于减少分区数只是窄依赖，因此不会涉及到shuffle操作。优化性的知识后面再看。

##### 处理预留的字符与关键字
